{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e86e10c-7db4-4342-8bf4-4e91c27d57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Avalanche: Continual Learning Framework\n",
    "## Benchmarks\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100, SplitCIFAR10\n",
    "from avalanche.benchmarks.datasets.torchvision_wrapper import CIFAR10\n",
    "from avalanche.benchmarks.scenarios import CLExperience\n",
    "from avalanche.benchmarks.utils.flat_data import ConstantSequence\n",
    "\n",
    "## Models\n",
    "from avalanche.models import (\n",
    "    MultiHeadClassifier,\n",
    "    MultiTaskModule,\n",
    "    MTSimpleMLP,\n",
    "    MTSimpleCNN,\n",
    "    PNN,\n",
    ")\n",
    "\n",
    "## Training Strategies\n",
    "from avalanche.training.supervised import Naive, EWC, LwF\n",
    "\n",
    "## Plugins and Logging\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, LRSchedulerPlugin\n",
    "\n",
    "## Evaluation Metrics\n",
    "from avalanche.evaluation.metrics import (\n",
    "    accuracy_metrics,\n",
    "    forgetting_metrics,\n",
    "    loss_metrics,\n",
    "    timing_metrics,\n",
    "    cpu_usage_metrics,\n",
    "    confusion_matrix_metrics,\n",
    "    disk_usage_metrics,\n",
    ")\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6364f5-3819-4eb2-8b81-f5eca3d1e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "import os\n",
    "\n",
    "if SAVE:\n",
    "    os.chdir('/home/uregina/DL_Project')\n",
    "    print(os.getcwd())\n",
    "\n",
    "# For saving the datasets/models/results/log files\n",
    "\n",
    "if SAVE:\n",
    "    DATASET_NAME = \"SplitCIFAR100\"\n",
    "    ROOT = Path(\"/home/uregina/DL_Project\")\n",
    "    DATA_ROOT = ROOT / DATASET_NAME\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485343c0-b8c4-4ed6-841a-6527423e45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 0\n",
    "\n",
    "DATASET_NAME = \"SplitCIFAR100\"\n",
    "NUM_CLASSES = {\n",
    "    \"SplitCIFAR100\": 100\n",
    "}\n",
    "\n",
    "SETTING = '1'# either '0' setting: 50 experiences (2 classes per experience) and 10 tasks or \n",
    "#'1' setting 10 experiences (10 classes per experience) and 5 tasks  corresponds to 10 and 4\n",
    "\n",
    "if SETTING == '1':\n",
    "    HPARAM = {\n",
    "    \"batch_size\": 128,        \n",
    "    \"num_epoch\": 10,           \n",
    "    \"start_lr\": 0.01,\n",
    "    \"alpha\": 1,   \n",
    "    \"temperature\": 2,   \n",
    "    \"NUM_EXP\" : 10,  \n",
    "    \"NUM_TASK\" : 4,  \n",
    "    }\n",
    "elif SETTING == '0':\n",
    "    HPARAM = {\n",
    "    \"batch_size\": 128,        \n",
    "    \"num_epoch\": 10,           \n",
    "    \"start_lr\": 0.01,\n",
    "    \"alpha\": 1,   \n",
    "    \"temperature\": 2,   \n",
    "    \"NUM_EXP\" : 50,  \n",
    "    \"NUM_TASK\" : 9,  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94b0356-ab17-4738-ac27-7b62875aaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # Adjusting the first Conv2d to take 192 channels as input (from EfficientNet b0 output)\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolution: 192 input channels to 120 output channels (reduce parameters)\n",
    "            nn.Conv2d(192, 120, kernel_size=1, stride=1, padding=0),  # 1x1 kernel reduces parameters\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Second convolution: 128 input channels to 64 output channels (further reduce)\n",
    "            nn.Conv2d(120, 64, kernel_size=1, stride=1, padding=0),  # 1x1 kernel reduces parameters\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Ensure fixed spatial size of 1x1 using AdaptiveAvgPool2d\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Dropout(p=0.25),\n",
    "        )\n",
    "\n",
    "        # Classifier will take the 64 channels (final output from features)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MTCustomCNN(CustomCNN, MultiTaskModule):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    with multi-head classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = MultiHeadClassifier(64)\n",
    "\n",
    "\n",
    "    def forward(self, x, task_labels):\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for classifier\n",
    "        x = self.classifier(x, task_labels)\n",
    "        return x\n",
    "\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "class EfficientNetCNN(MultiTaskModule):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetCNN, self).__init__()\n",
    "        # Use the updated `weights` parameter instead of `pretrained`\n",
    "        self.model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze all parameters in the model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Extract layers up to block 7\n",
    "        self.block7 = nn.Sequential(\n",
    "            *list(self.model.features[:7])  # Extract up to and including block 7\n",
    "        )\n",
    "        \n",
    "        # Custom classifier\n",
    "        self.classifier = MTCustomCNN()\n",
    "\n",
    "        # Freeze EfficientNet parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward_single_task(self, x, task_id):\n",
    "        x = self.block7(x)\n",
    "        out = self.classifier(x, task_id)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "168eead1-d406-49d2-9c73-f14b131a075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_order = [73, 48, 17, 32, 11, 62, 68, 92, 91, 3, 77, 79, 43, 88, 47, 82, 13, 78, 70, 90, 12, 37, 2, 76, 84, 98, 59, 96, 52, 93, 26, 45, 20, 46, 29, 56, 97, 44, 35, 58, 5, 8, 94, 54, 67, 27, 99, 1, 25, 42, 0, 4, 6, 7, 9, 10, 14, 15, 16, 18, 19, 21, 22, 23, 24, 28, 30, 31, 33, 34, 36, 38, 39, 40, 41, 49, 50, 51, 53, 55, 57, 60, 61, 63, 64, 65, 66, 69, 71, 72, 74, 75, 80, 81, 83, 85, 86, 87, 89, 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a483dd24-06fd-4015-adde-a2785caa4d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "benchmark = SplitCIFAR100(\n",
    "    n_experiences = HPARAM[\"NUM_EXP\"],          \n",
    "    return_task_id = True,\n",
    "    fixed_class_order= class_order, \n",
    "    train_transform = train_transform,\n",
    "    eval_transform = test_transform,\n",
    "    \n",
    ")\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(\n",
    "        num_classes=NUM_CLASSES[DATASET_NAME], save_image=False, stream=True\n",
    "    ),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=interactive_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a671a4d6-9b28-4b58-bde4-4c4713f24810",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'EfficientNetCNN'\n",
    "RUN = '0'                    #Multiple runs 0,1,2\n",
    "model = EfficientNetCNN()\n",
    "\n",
    "optimizer = Adam(model.parameters(), HPARAM[\"start_lr\"])\n",
    "\n",
    "cl_strategy = LwF(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    train_mb_size=HPARAM[\"batch_size\"],\n",
    "    train_epochs=HPARAM[\"num_epoch\"],\n",
    "    eval_mb_size=HPARAM[\"batch_size\"],\n",
    "    alpha=HPARAM[\"alpha\"],              # LwF parameter\n",
    "    temperature=HPARAM[\"temperature\"],  # LwF parameter\n",
    "    evaluator=eval_plugin,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "if SAVE:\n",
    "    DATA_ROOT = ROOT / DATASET_NAME / MODEL_NAME / RUN\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27668537-37e1-4091-aacb-c8038bd728c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [32, 3, 68, 73, 11, 48, 17, 91, 92, 62]\n",
      "-- >> Start of training phase << --\n",
      " 12%|█▎        | 5/40 [00:02<00:13,  2.55it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart of experience: \u001b[39m\u001b[38;5;124m\"\u001b[39m, experience\u001b[38;5;241m.\u001b[39mcurrent_experience)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Classes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, experience\u001b[38;5;241m.\u001b[39mclasses_in_this_experience)\n\u001b[0;32m----> 6\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcl_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing accuracy on the whole test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:213\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    212\u001b[0m ):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base.py:163\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:339\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/update_type/sgd_update.py:38\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_backward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_backward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:261\u001b[0m, in \u001b[0;36mBaseSGDTemplate.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the backward pass.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting experiment...\")\n",
    "results_dict = {}  # Use a dictionary instead of a list\n",
    "for index, experience in enumerate(benchmark.train_stream):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "    res = cl_strategy.train(experience)\n",
    "    print(\"Training completed\")\n",
    "    print(\"Computing accuracy on the whole test\")\n",
    "    results_dict[index] = cl_strategy.eval(benchmark.test_stream)  # Use the index as the key\n",
    "    if index == HPARAM[\"NUM_TASK\"]:\n",
    "        #results_dict[index] = cl_strategy.eval(benchmark.test_stream)\n",
    "        break\n",
    "\n",
    "print(\"Experiment completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7cdc6-256a-4275-80ba-cd64a8b49c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    file_name = f\"{MODEL_NAME}_{DATASET_NAME}_{RUN}_results.txt\"\n",
    "    file_path = ROOT / DATASET_NAME / MODEL_NAME / RUN / file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "        file.write(f\"Dataset: {DATASET_NAME}\\n\")\n",
    "        file.write(f\"Run: {RUN}\\n\")\n",
    "        file.write(f\"Setting: {SETTING}\\n\")    \n",
    "        file.write(\"\\nResults Dictionary:\\n\")\n",
    "        file.write(\"--------------------------------------------------\\n\")\n",
    "        for key, value in results_dict.items():\n",
    "            file.write(f\"Experience {key}:\\n\")\n",
    "            for metric, metric_value in value.items():\n",
    "                # Convert tensors to lists for saving\n",
    "                if isinstance(metric_value, torch.Tensor):\n",
    "                    metric_value = metric_value.tolist()\n",
    "                file.write(f\"  {metric}: {metric_value}\\n\")\n",
    "            file.write(\"--------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_name)",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
