{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ba4689-90de-4587-9dbd-b8c5bb4871d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Avalanche: Continual Learning Framework\n",
    "## Benchmarks\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100, SplitCIFAR10\n",
    "from avalanche.benchmarks.datasets.torchvision_wrapper import CIFAR10\n",
    "from avalanche.benchmarks.scenarios import CLExperience\n",
    "from avalanche.benchmarks.utils.flat_data import ConstantSequence\n",
    "\n",
    "## Models\n",
    "from avalanche.models import (\n",
    "    MultiHeadClassifier,\n",
    "    MultiTaskModule,\n",
    "    MTSimpleMLP,\n",
    "    MTSimpleCNN,\n",
    "    PNN,\n",
    ")\n",
    "\n",
    "## Training Strategies\n",
    "from avalanche.training.supervised import Naive, EWC, LwF\n",
    "\n",
    "## Plugins and Logging\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, LRSchedulerPlugin\n",
    "\n",
    "## Evaluation Metrics\n",
    "from avalanche.evaluation.metrics import (\n",
    "    accuracy_metrics,\n",
    "    forgetting_metrics,\n",
    "    loss_metrics,\n",
    "    timing_metrics,\n",
    "    cpu_usage_metrics,\n",
    "    confusion_matrix_metrics,\n",
    "    disk_usage_metrics,\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6aed10-24a7-48a1-917b-4dd29bb44211",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "import os\n",
    "\n",
    "if SAVE:\n",
    "    os.chdir('/home/uregina/DL_Project')\n",
    "    print(os.getcwd())\n",
    "\n",
    "# For saving the datasets/models/results/log files\n",
    "\n",
    "if SAVE:\n",
    "    DATASET_NAME = \"SplitCIFAR10\"\n",
    "    ROOT = Path(\"/home/uregina/DL_Project\")\n",
    "    DATA_ROOT = ROOT / DATASET_NAME\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4954af8-cc17-4f84-b0a4-a55374b61503",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 0\n",
    "\n",
    "DATASET_NAME = \"SplitCIFAR10\"\n",
    "NUM_CLASSES = {\n",
    "    \"SplitCIFAR10\": 10\n",
    "}\n",
    "\n",
    "# Define hyperparameters/scheduler/augmentation\n",
    "HPARAM = {\n",
    "    \"batch_size\": 128,        #CHANGE\n",
    "    \"num_epoch\": 4,           #CHANGE if too much\n",
    "    \"start_lr\": 0.01,\n",
    "    \"alpha\": 1,   #change based model\n",
    "    \"temperature\": 2,   #change based model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9e7c6a-eb50-4ac0-a268-ff76e6b20b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.models.dynamic_modules import DynamicModule\n",
    "\n",
    "class IncrementalCNNClassifier(DynamicModule):\n",
    "    \"\"\"\n",
    "    Output layer that incrementally adds units whenever new classes are\n",
    "    encountered.\n",
    "\n",
    "    Typically used in class-incremental benchmarks where the number of\n",
    "    classes grows over time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_out_features=2, masking=True, mask_value=-1000):\n",
    "        \"\"\"\n",
    "        :param in_features: number of input features.\n",
    "        :param initial_out_features: initial number of classes (can be\n",
    "            dynamically expanded).\n",
    "        :param masking: whether unused units should be masked (default=True).\n",
    "        :param mask_value: the value used for masked units (default=-1000).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.mask_value = mask_value\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 64, kernel_size=1, stride=1, padding=0),  # 1x1 kernel\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  \n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Add a new fully connected layer\n",
    "        self.classifier = nn.Linear(64, initial_out_features)\n",
    "\n",
    "        au_init = torch.zeros(initial_out_features, dtype=torch.bool)\n",
    "        self.register_buffer('active_units', au_init)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adaptation(self, experience: CLExperience):\n",
    "        \"\"\"If `dataset` contains unseen classes the classifier is expanded.\n",
    "\n",
    "        :param experience: data from the current experience.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        in_features = self.classifier.in_features\n",
    "        old_nclasses = self.classifier.out_features\n",
    "        curr_classes = experience.classes_in_this_experience\n",
    "        new_nclasses = max(\n",
    "            self.classifier.out_features, max(curr_classes) + 1\n",
    "        )\n",
    "\n",
    "        # update active_units mask\n",
    "        if self.masking:\n",
    "            if old_nclasses != new_nclasses:  # expand active_units mask\n",
    "                old_act_units = self.active_units\n",
    "                self.active_units = torch.zeros(new_nclasses, dtype=torch.bool)\n",
    "                self.active_units[:old_act_units.shape[0]] = old_act_units\n",
    "            # update with new active classes\n",
    "            if self.training:\n",
    "                self.active_units[curr_classes] = 1\n",
    "\n",
    "        # update classifier weights\n",
    "        if old_nclasses == new_nclasses:\n",
    "            return\n",
    "        old_w, old_b = self.classifier.weight, self.classifier.bias\n",
    "        self.classifier = torch.nn.Linear(in_features, new_nclasses)\n",
    "        self.classifier.weight[:old_nclasses] = old_w\n",
    "        self.classifier.bias[:old_nclasses] = old_b\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"Compute the output given the input `x`. This module does not use\n",
    "        the task label.\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        out = self.classifier(x)\n",
    "        if self.masking:\n",
    "            out[..., torch.logical_not(self.active_units)] = self.mask_value\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadCNNClassifier(MultiTaskModule):\n",
    "    \"\"\"Multi-head classifier with separate heads for each task.\n",
    "\n",
    "    Typically used in task-incremental benchmarks where task labels are\n",
    "    available and provided to the model.\n",
    "\n",
    "    .. note::\n",
    "        Each output head may have a different shape, and the number of\n",
    "        classes can be determined automatically.\n",
    "\n",
    "        However, since pytorch doest not support jagged tensors, when you\n",
    "        compute a minibatch's output you must ensure that each sample\n",
    "        has the same output size, otherwise the model will fail to\n",
    "        concatenate the samples together.\n",
    "\n",
    "        These can be easily ensured in two possible ways:\n",
    "\n",
    "        - each minibatch contains a single task, which is the case in most\n",
    "            common benchmarks in Avalanche. Some exceptions to this setting\n",
    "            are multi-task replay or cumulative strategies.\n",
    "        - each head has the same size, which can be enforced by setting a\n",
    "            large enough `initial_out_features`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_out_features=2,\n",
    "                 masking=True, mask_value=-1000):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param in_features: number of input features.\n",
    "        :param initial_out_features: initial number of classes (can be\n",
    "            dynamically expanded).\n",
    "        :param masking: whether unused units should be masked (default=True).\n",
    "        :param mask_value: the value used for masked units (default=-1000).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.mask_value = mask_value\n",
    "        self.starting_out_features = initial_out_features\n",
    "        self.classifiers = torch.nn.ModuleDict()\n",
    "\n",
    "        # needs to create the first head because pytorch optimizers\n",
    "        # fail when model.parameters() is empty.\n",
    "        # masking in IncrementalClassifier is unaware of task labels\n",
    "        # so we do masking here instead.\n",
    "        first_head = IncrementalCNNClassifier(\n",
    "         self.starting_out_features, masking=False\n",
    "        )\n",
    "        self.classifiers[\"0\"] = first_head\n",
    "        self.max_class_label = max(self.max_class_label, initial_out_features)\n",
    "\n",
    "        au_init = torch.zeros(initial_out_features, dtype=torch.bool)\n",
    "        self.register_buffer('active_units_T0', au_init)\n",
    "\n",
    "\n",
    "    def adaptation(self, experience: CLExperience):\n",
    "        \"\"\"If `dataset` contains new tasks, a new head is initialized.\n",
    "\n",
    "        :param experience: data from the current experience.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super().adaptation(experience)\n",
    "        curr_classes = experience.classes_in_this_experience\n",
    "        task_labels = experience.task_labels\n",
    "        if isinstance(task_labels, ConstantSequence):\n",
    "            # task label is unique. Don't check duplicates.\n",
    "            task_labels = [task_labels[0]]\n",
    "\n",
    "        for tid in set(task_labels):\n",
    "            # head adaptation\n",
    "            tid = str(tid)  # need str keys\n",
    "            if tid not in self.classifiers:  # create new head\n",
    "                new_head = IncrementalCNNClassifier(self.starting_out_features\n",
    "                )\n",
    "                self.classifiers[tid] = new_head\n",
    "\n",
    "                au_init = torch.zeros(self.starting_out_features,\n",
    "                                      dtype=torch.bool)\n",
    "                self.register_buffer(f'active_units_T{tid}', au_init)\n",
    "\n",
    "            self.classifiers[tid].adaptation(experience)\n",
    "\n",
    "            # update active_units mask for the current task\n",
    "            if self.masking:\n",
    "                # TODO: code below assumes a single task for each experience\n",
    "                # it should be easy to generalize but it may be slower.\n",
    "                if len(task_labels) > 1:\n",
    "                    raise NotImplementedError(\n",
    "                        \"Multi-Head unit masking is not supported when \"\n",
    "                        \"experiences have multiple task labels. Set \"\n",
    "                        \"masking=False in your \"\n",
    "                        \"MultiHeadClassifier to disable masking.\")\n",
    "\n",
    "                au_name = f'active_units_T{tid}'\n",
    "                curr_head = self.classifiers[str(tid)]\n",
    "                old_nunits = self._buffers[au_name].shape[0]\n",
    "\n",
    "                new_nclasses = max(\n",
    "                    curr_head.classifier.out_features, max(curr_classes) + 1\n",
    "                )\n",
    "                if old_nunits != new_nclasses:  # expand active_units mask\n",
    "                    old_act_units = self._buffers[au_name]\n",
    "                    self._buffers[au_name] = torch.zeros(new_nclasses,\n",
    "                                                         dtype=torch.bool)\n",
    "                    self._buffers[au_name][:old_act_units.shape[0]] = \\\n",
    "                        old_act_units\n",
    "                # update with new active classes\n",
    "                if self.training:\n",
    "                    self._buffers[au_name][curr_classes] = 1\n",
    "\n",
    "    def forward_single_task(self, x, task_label):\n",
    "        \"\"\"compute the output given the input `x`. This module uses the task\n",
    "        label to activate the correct head.\n",
    "\n",
    "        :param x:\n",
    "        :param task_label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.classifiers[str(task_label)](x)\n",
    "        if self.masking:\n",
    "            au_name = f'active_units_T{task_label}'\n",
    "            curr_au = self._buffers[au_name]\n",
    "            nunits, oldsize = out.shape[-1], curr_au.shape[0]\n",
    "            if oldsize < nunits:  # we have to update the mask\n",
    "                old_mask = self._buffers[au_name]\n",
    "                self._buffers[au_name] = torch.zeros(nunits, dtype=torch.bool)\n",
    "                self._buffers[au_name][:oldsize] = old_mask\n",
    "                curr_au = self._buffers[au_name]\n",
    "            out[..., torch.logical_not(curr_au)] = self.mask_value\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47676fa9-167d-4085-b91b-7da118924a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "benchmark = SplitCIFAR10(\n",
    "    n_experiences = 5,          \n",
    "    return_task_id = True  \n",
    ")\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(\n",
    "        num_classes=NUM_CLASSES[DATASET_NAME], save_image=False, stream=True\n",
    "    ),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=interactive_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ea718f-a7fd-4eb3-aa2c-de5b4cf3175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PCNN'\n",
    "RUN = '0'                    #Multiple runs 0,1,2\n",
    "model = MultiHeadCNNClassifier()\n",
    "\n",
    "optimizer = Adam(model.parameters(), HPARAM[\"start_lr\"])\n",
    "\n",
    "cl_strategy = LwF(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    train_mb_size=HPARAM[\"batch_size\"],\n",
    "    train_epochs=HPARAM[\"num_epoch\"],\n",
    "    eval_mb_size=HPARAM[\"batch_size\"],\n",
    "    alpha=HPARAM[\"alpha\"],              # LwF parameter\n",
    "    temperature=HPARAM[\"temperature\"],  # LwF parameter\n",
    "    evaluator=eval_plugin,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "if SAVE:\n",
    "    DATA_ROOT = ROOT / DATASET_NAME / MODEL_NAME / RUN\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "545c7fcb-b542-4298-91c4-43bceca0be3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [1, 4]\n",
      "-- >> Start of training phase << --\n",
      " 84%|████████▎ | 66/79 [00:06<00:01, 10.80it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:213\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    212\u001b[0m ):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base.py:163\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:339\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/update_type/sgd_update.py:19\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Training epoch.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    :param kwargs:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stop_training\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/data_loader.py:199\u001b[0m, in \u001b[0;36mMultiDatasetDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_loader\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_loader()\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/data.py:355\u001b[0m, in \u001b[0;36mAvalancheDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m: TAvalancheDataset, idx: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mslice\u001b[39m]\n\u001b[1;32m    354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[T_co, TAvalancheDataset]:\n\u001b[0;32m--> 355\u001b[0m     elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m da \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attributes\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m da\u001b[38;5;241m.\u001b[39muse_in_getitem:\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/data.py:558\u001b[0m, in \u001b[0;36m_FlatDataWithTransform.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m: TDataWTransform, idx: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mslice\u001b[39m]\n\u001b[1;32m    556\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[T_co, TDataWTransform]:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m--> 558\u001b[0m         elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_recursive_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_groups\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_group\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/data.py:544\u001b[0m, in \u001b[0;36m_FlatDataWithTransform._getitem_recursive_call\u001b[0;34m(self, idx, group_name)\u001b[0m\n\u001b[1;32m    542\u001b[0m dd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets[dataset_idx]\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dd, _FlatDataWithTransform):\n\u001b[0;32m--> 544\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_recursive_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     element \u001b[38;5;241m=\u001b[39m dd[idx]\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/data.py:544\u001b[0m, in \u001b[0;36m_FlatDataWithTransform._getitem_recursive_call\u001b[0;34m(self, idx, group_name)\u001b[0m\n\u001b[1;32m    542\u001b[0m dd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets[dataset_idx]\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dd, _FlatDataWithTransform):\n\u001b[0;32m--> 544\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_recursive_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     element \u001b[38;5;241m=\u001b[39m dd[idx]\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/data.py:551\u001b[0m, in \u001b[0;36m_FlatDataWithTransform._getitem_recursive_call\u001b[0;34m(self, idx, group_name)\u001b[0m\n\u001b[1;32m    549\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frozen_transform_groups(element, group_name\u001b[38;5;241m=\u001b[39mgroup_name)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m element\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/transform_groups.py:131\u001b[0m, in \u001b[0;36mTransformGroups.__call__\u001b[0;34m(self, group_name, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m     element[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m curr_t(element[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_t\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m element\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/benchmarks/utils/transforms.py:279\u001b[0m, in \u001b[0;36mTupleTransform.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms):\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         args_list[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_list\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torchvision/transforms/transforms.py:681\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    678\u001b[0m     padding \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m height]\n\u001b[1;32m    679\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(img, padding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[0;32m--> 681\u001b[0m i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcrop(img, i, j, h, w)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torchvision/transforms/transforms.py:645\u001b[0m, in \u001b[0;36mRandomCrop.get_params\u001b[0;34m(img, output_size)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m==\u001b[39m tw \u001b[38;5;129;01mand\u001b[39;00m h \u001b[38;5;241m==\u001b[39m th:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, h, w\n\u001b[0;32m--> 645\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    646\u001b[0m j \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, w \u001b[38;5;241m-\u001b[39m tw \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m i, j, th, tw\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Starting experiment...\")\n",
    "results_dict = {}  # Use a dictionary instead of a list\n",
    "for index, experience in enumerate(benchmark.train_stream):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "    res = cl_strategy.train(experience)\n",
    "    print(\"Training completed\")\n",
    "    print(\"Computing accuracy on the whole test\")\n",
    "    results_dict[index] = cl_strategy.eval(benchmark.test_stream)  # Use the index as the key\n",
    "\n",
    "print(\"Experiment completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "541ddd77-f4bd-4a80-b711-b4a8e1a1b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    file_name = f\"{MODEL_NAME}_{DATASET_NAME}_{RUN}_results.txt\"\n",
    "    file_path = ROOT / DATASET_NAME / MODEL_NAME / RUN / file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "        file.write(f\"Dataset: {DATASET_NAME}\\n\")\n",
    "        file.write(f\"Run: {RUN}\\n\") \n",
    "        file.write(\"\\nResults Dictionary:\\n\")\n",
    "        file.write(\"--------------------------------------------------\\n\")\n",
    "        for key, value in results_dict.items():\n",
    "            file.write(f\"Experience {key}:\\n\")\n",
    "            for metric, metric_value in value.items():\n",
    "                # Convert tensors to lists for saving\n",
    "                if isinstance(metric_value, torch.Tensor):\n",
    "                    metric_value = metric_value.tolist()\n",
    "                file.write(f\"  {metric}: {metric_value}\\n\")\n",
    "            file.write(\"--------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_name)",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
