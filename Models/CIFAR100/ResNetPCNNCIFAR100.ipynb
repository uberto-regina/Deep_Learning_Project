{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b03baa3-d6ca-47ea-b168-c8e81a10130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Avalanche: Continual Learning Framework\n",
    "## Benchmarks\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100, SplitCIFAR10\n",
    "from avalanche.benchmarks.datasets.torchvision_wrapper import CIFAR10\n",
    "from avalanche.benchmarks.scenarios import CLExperience\n",
    "from avalanche.benchmarks.utils.flat_data import ConstantSequence\n",
    "\n",
    "## Models\n",
    "from avalanche.models import (\n",
    "    MultiHeadClassifier,\n",
    "    MultiTaskModule,\n",
    "    MTSimpleMLP,\n",
    "    MTSimpleCNN,\n",
    "    PNN,\n",
    ")\n",
    "\n",
    "## Training Strategies\n",
    "from avalanche.training.supervised import Naive, EWC, LwF\n",
    "\n",
    "## Plugins and Logging\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, LRSchedulerPlugin\n",
    "\n",
    "## Evaluation Metrics\n",
    "from avalanche.evaluation.metrics import (\n",
    "    accuracy_metrics,\n",
    "    forgetting_metrics,\n",
    "    loss_metrics,\n",
    "    timing_metrics,\n",
    "    cpu_usage_metrics,\n",
    "    confusion_matrix_metrics,\n",
    "    disk_usage_metrics,\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940d0658-e1c5-4039-a28b-2c32c56dbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "import os\n",
    "\n",
    "if SAVE:\n",
    "    os.chdir('/home/uregina/DL_Project')\n",
    "    print(os.getcwd())\n",
    "\n",
    "# For saving the datasets/models/results/log files\n",
    "\n",
    "if SAVE:\n",
    "    DATASET_NAME = \"SplitCIFAR100\"\n",
    "    ROOT = Path(\"/home/uregina/DL_Project\")\n",
    "    DATA_ROOT = ROOT / DATASET_NAME\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455969b0-9872-4c91-b78b-8e11684a67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 0\n",
    "\n",
    "DATASET_NAME = \"SplitCIFAR100\"\n",
    "NUM_CLASSES = {\n",
    "    \"SplitCIFAR100\": 100\n",
    "}\n",
    "\n",
    "# Define hyperparameters/scheduler/augmentation\n",
    "\n",
    "SETTING = '1'# either '0' setting: 50 experiences (2 classes per experience) and 10 tasks or \n",
    "#'1' setting 10 experiences (10 classes per experience) and 5 tasks  corresponds to 10 and 4\n",
    "\n",
    "if SETTING == '1':\n",
    "    HPARAM = {\n",
    "    \"batch_size\": 128,        \n",
    "    \"num_epoch\": 10,           \n",
    "    \"start_lr\": 0.01,\n",
    "    \"alpha\": 1,   \n",
    "    \"temperature\": 2,   \n",
    "    \"NUM_EXP\" : 10,  \n",
    "    \"NUM_TASK\" : 4,  \n",
    "    }\n",
    "elif SETTING == '0':\n",
    "    HPARAM = {\n",
    "    \"batch_size\": 128,        \n",
    "    \"num_epoch\": 10,           \n",
    "    \"start_lr\": 0.01,\n",
    "    \"alpha\": 1,   \n",
    "    \"temperature\": 2,   \n",
    "    \"NUM_EXP\" : 50,  \n",
    "    \"NUM_TASK\" : 9,  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b2e279-1f77-4ba0-9630-c6edfbf005a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.models.dynamic_modules import DynamicModule\n",
    "\n",
    "class ResNetIncrementalCNNClassifier(DynamicModule):\n",
    "    \"\"\"\n",
    "    Output layer that incrementally adds units whenever new classes are\n",
    "    encountered.\n",
    "\n",
    "    Typically used in class-incremental benchmarks where the number of\n",
    "    classes grows over time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, initial_out_features=2, masking=True, mask_value=-10000000):\n",
    "        \"\"\"\n",
    "        :param in_features: number of input features.\n",
    "        :param initial_out_features: initial number of classes (can be\n",
    "            dynamically expanded).\n",
    "        :param masking: whether unused units should be masked (default=True).\n",
    "        :param mask_value: the value used for masked units (default=-1000).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.mask_value = mask_value\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolution: 512 input channels to 256 output channels\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),  # Maintain spatial resolution\n",
    "            nn.BatchNorm2d(256),  # Add BatchNorm for stability\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Second convolution: 256 input channels to 128 output channels\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),  # Still maintain spatial resolution\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Additional layer: 128 input channels to 128 output channels\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Enhance feature extraction\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Third convolution: 128 input channels to 64 output channels\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),  # Spatial resolution retained\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Additional layer: 64 input channels to 66 output channels\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Slightly increase channels\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Global Average Pooling for spatial size reduction\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "\n",
    "            # Dropout for regularization\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        \n",
    "        # Classifier will take the 64 channels (final output from features)\n",
    "        self.classifier = nn.Linear(64, initial_out_features)\n",
    " \n",
    "\n",
    "        au_init = torch.zeros(initial_out_features, dtype=torch.bool)\n",
    "        self.register_buffer('active_units', au_init)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adaptation(self, experience: CLExperience):\n",
    "        \"\"\"If `dataset` contains unseen classes the classifier is expanded.\n",
    "\n",
    "        :param experience: data from the current experience.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        in_features = self.classifier.in_features\n",
    "        old_nclasses = self.classifier.out_features\n",
    "        curr_classes = experience.classes_in_this_experience\n",
    "        #print(\"curr_classes\", old_nclasses, curr_classes)\n",
    "        new_nclasses = max(\n",
    "            self.classifier.out_features, max(curr_classes) + 1\n",
    "        )\n",
    "\n",
    "        # update active_units mask\n",
    "        if self.masking:\n",
    "            if old_nclasses != new_nclasses:  # expand active_units mask\n",
    "                old_act_units = self.active_units\n",
    "                self.active_units = torch.zeros(new_nclasses, dtype=torch.bool)\n",
    "                self.active_units[:old_act_units.shape[0]] = old_act_units\n",
    "            # update with new active classes\n",
    "            if self.training:\n",
    "                self.active_units[curr_classes] = 1\n",
    "\n",
    "        # update classifier weights\n",
    "        if old_nclasses == new_nclasses:\n",
    "            return\n",
    "        old_w, old_b = self.classifier.weight, self.classifier.bias\n",
    "        #print(\"old\", self.classifier.weight[:old_nclasses], self.classifier.bias[:old_nclasses])\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(in_features, new_nclasses)\n",
    "        self.classifier.weight[:old_nclasses] = old_w\n",
    "        self.classifier.bias[:old_nclasses] = old_b\n",
    "        #print(\"new\", self.classifier.weight, self.classifier.bias)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"Compute the output given the input `x`. This module does not use\n",
    "        the task label.\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x  = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for classifier\n",
    "        out = self.classifier(x)\n",
    "        if self.masking:\n",
    "            out[..., torch.logical_not(self.active_units)] = self.mask_value\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNetMultiHeadCNNClassifier(MultiTaskModule):\n",
    "    \"\"\"Multi-head classifier with separate heads for each task.\n",
    "\n",
    "    Typically used in task-incremental benchmarks where task labels are\n",
    "    available and provided to the model.\n",
    "\n",
    "    .. note::\n",
    "        Each output head may have a different shape, and the number of\n",
    "        classes can be determined automatically.\n",
    "\n",
    "        However, since pytorch doest not support jagged tensors, when you\n",
    "        compute a minibatch's output you must ensure that each sample\n",
    "        has the same output size, otherwise the model will fail to\n",
    "        concatenate the samples together.\n",
    "\n",
    "        These can be easily ensured in two possible ways:\n",
    "\n",
    "        - each minibatch contains a single task, which is the case in most\n",
    "            common benchmarks in Avalanche. Some exceptions to this setting\n",
    "            are multi-task replay or cumulative strategies.\n",
    "        - each head has the same size, which can be enforced by setting a\n",
    "            large enough `initial_out_features`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, initial_out_features=2,\n",
    "                 masking=True, mask_value=-10000000):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param in_features: number of input features.\n",
    "        :param initial_out_features: initial number of classes (can be\n",
    "            dynamically expanded).\n",
    "        :param masking: whether unused units should be masked (default=True).\n",
    "        :param mask_value: the value used for masked units (default=-1000).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.mask_value = mask_value\n",
    "        self.in_features = in_features\n",
    "        self.starting_out_features = initial_out_features\n",
    "        self.classifiers = torch.nn.ModuleDict()\n",
    "\n",
    "        # needs to create the first head because pytorch optimizers\n",
    "        # fail when model.parameters() is empty.\n",
    "        # masking in IncrementalClassifier is unaware of task labels\n",
    "        # so we do masking here instead.\n",
    "        first_head = ResNetIncrementalCNNClassifier(\n",
    "            self.in_features, self.starting_out_features, masking=False\n",
    "        )\n",
    "        self.classifiers[\"0\"] = first_head\n",
    "        self.max_class_label = max(self.max_class_label, initial_out_features)\n",
    "\n",
    "        au_init = torch.zeros(initial_out_features, dtype=torch.bool)\n",
    "        self.register_buffer('active_units_T0', au_init)\n",
    "\n",
    "\n",
    "    def adaptation(self, experience: CLExperience):\n",
    "        \"\"\"If `dataset` contains new tasks, a new head is initialized.\n",
    "\n",
    "        :param experience: data from the current experience.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super().adaptation(experience)\n",
    "        curr_classes = experience.classes_in_this_experience\n",
    "        task_labels = experience.task_labels\n",
    "        if isinstance(task_labels, ConstantSequence):\n",
    "            # task label is unique. Don't check duplicates.\n",
    "            task_labels = [task_labels[0]]\n",
    "\n",
    "        for tid in set(task_labels):\n",
    "            # head adaptation\n",
    "            tid = str(tid)  # need str keys\n",
    "            if tid not in self.classifiers:  # create new head\n",
    "                new_head = ResNetIncrementalCNNClassifier(\n",
    "                    self.in_features, self.starting_out_features\n",
    "                )\n",
    "                self.classifiers[tid] = new_head\n",
    "\n",
    "                au_init = torch.zeros(self.starting_out_features,\n",
    "                                      dtype=torch.bool)\n",
    "                self.register_buffer(f'active_units_T{tid}', au_init)\n",
    "\n",
    "            self.classifiers[tid].adaptation(experience)\n",
    "\n",
    "            # update active_units mask for the current task\n",
    "            if self.masking:\n",
    "                # TODO: code below assumes a single task for each experience\n",
    "                # it should be easy to generalize but it may be slower.\n",
    "                if len(task_labels) > 1:\n",
    "                    raise NotImplementedError(\n",
    "                        \"Multi-Head unit masking is not supported when \"\n",
    "                        \"experiences have multiple task labels. Set \"\n",
    "                        \"masking=False in your \"\n",
    "                        \"MultiHeadClassifier to disable masking.\")\n",
    "\n",
    "                au_name = f'active_units_T{tid}'\n",
    "                curr_head = self.classifiers[str(tid)]\n",
    "                old_nunits = self._buffers[au_name].shape[0]\n",
    "\n",
    "                new_nclasses = max(\n",
    "                    curr_head.classifier.out_features, max(curr_classes) + 1\n",
    "                )\n",
    "                if old_nunits != new_nclasses:  # expand active_units mask\n",
    "                    old_act_units = self._buffers[au_name]\n",
    "                    self._buffers[au_name] = torch.zeros(new_nclasses,\n",
    "                                                         dtype=torch.bool)\n",
    "                    self._buffers[au_name][:old_act_units.shape[0]] = \\\n",
    "                        old_act_units\n",
    "                # update with new active classes\n",
    "                if self.training:\n",
    "                    self._buffers[au_name][curr_classes] = 1\n",
    "\n",
    "    def forward_single_task(self, x, task_label):\n",
    "        \"\"\"compute the output given the input `x`. This module uses the task\n",
    "        label to activate the correct head.\n",
    "\n",
    "        :param x:\n",
    "        :param task_label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.classifiers[str(task_label)](x)\n",
    "        if self.masking:\n",
    "            au_name = f'active_units_T{task_label}'\n",
    "            curr_au = self._buffers[au_name]\n",
    "            nunits, oldsize = out.shape[-1], curr_au.shape[0]\n",
    "            if oldsize < nunits:  # we have to update the mask\n",
    "                old_mask = self._buffers[au_name]\n",
    "                self._buffers[au_name] = torch.zeros(nunits, dtype=torch.bool)\n",
    "                self._buffers[au_name][:oldsize] = old_mask\n",
    "                curr_au = self._buffers[au_name]\n",
    "            out[..., torch.logical_not(curr_au)] = self.mask_value\n",
    "        return out\n",
    "\n",
    "class ResNetPCNN(MultiTaskModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "        self.resnet_backbone = nn.Sequential(\n",
    "            self.resnet18.conv1,    # Initial conv layer\n",
    "            self.resnet18.bn1,      # Batch normalization\n",
    "            self.resnet18.relu,     # Activation\n",
    "            self.resnet18.maxpool,  # Max pooling\n",
    "            self.resnet18.layer1,   # Layer 1\n",
    "            self.resnet18.layer2,   # Layer 2\n",
    "            self.resnet18.layer3,    # Layer 3 (final output layer)\n",
    "            self.resnet18.layer4\n",
    "        )\n",
    "\n",
    "        self.classifier = ResNetMultiHeadCNNClassifier(512)\n",
    "\n",
    "        for param in self.resnet_backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.resnet18.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward_single_task(self, x, task_id):\n",
    "        \n",
    "        features = self.resnet_backbone(x)\n",
    "        \n",
    "        return self.classifier(features, task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35603c4-14fd-463d-aca1-222200475e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_order = [73, 48, 17, 32, 11, 62, 68, 92, 91, 3, 77, 79, 43, 88, 47, 82, 13, 78, 70, 90, 12, 37, 2, 76, 84, 98, 59, 96, 52, 93, 26, 45, 20, 46, 29, 56, 97, 44, 35, 58, 5, 8, 94, 54, 67, 27, 99, 1, 25, 42, 0, 4, 6, 7, 9, 10, 14, 15, 16, 18, 19, 21, 22, 23, 24, 28, 30, 31, 33, 34, 36, 38, 39, 40, 41, 49, 50, 51, 53, 55, 57, 60, 61, 63, 64, 65, 66, 69, 71, 72, 74, 75, 80, 81, 83, 85, 86, 87, 89, 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3b8290-e94c-401c-8411-9f639a4df94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "benchmark = SplitCIFAR100(\n",
    "    n_experiences = HPARAM[\"NUM_EXP\"],          \n",
    "    return_task_id = True,\n",
    "    fixed_class_order= class_order, \n",
    "    train_transform = train_transform,\n",
    "    eval_transform = test_transform,  \n",
    ")\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(\n",
    "        num_classes=NUM_CLASSES[DATASET_NAME], save_image=False, stream=True\n",
    "    ),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=interactive_logger,\n",
    ")\n",
    "\n",
    "if SAVE:\n",
    "    DATA_ROOT = ROOT / DATASET_NAME / MODEL_NAME / RUN\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0933e6-ca6f-41ea-ab7d-b157f9791270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uregina/env_name/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/uregina/env_name/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'ResNetPCNN'\n",
    "RUN = '0'                    #Multiple runs 0,1,2\n",
    "model =  ResNetPCNN()\n",
    "\n",
    "optimizer = Adam(model.parameters(), HPARAM[\"start_lr\"])\n",
    "\n",
    "cl_strategy = LwF(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    train_mb_size=HPARAM[\"batch_size\"],\n",
    "    train_epochs=HPARAM[\"num_epoch\"],\n",
    "    eval_mb_size=HPARAM[\"batch_size\"],\n",
    "    alpha=HPARAM[\"alpha\"],              # LwF parameter\n",
    "    temperature=HPARAM[\"temperature\"],  # LwF parameter\n",
    "    evaluator=eval_plugin,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4cb0ec1-f7f7-464a-a0bd-79bc9c1cc0ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [32, 3, 68, 73, 11, 48, 17, 91, 92, 62]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 40/40 [00:12<00:00,  3.19it/s]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 450.3652\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 450.3652\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0865\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0216\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 12.5500\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6310\n",
      "100%|██████████| 40/40 [00:12<00:00,  3.23it/s]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 450.3652\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 450.3652\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.5497\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0079\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 12.3644\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8128\n",
      " 68%|██████▊   | 27/40 [00:08<00:04,  3.23it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:213\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    212\u001b[0m ):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base.py:163\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:339\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/update_type/sgd_update.py:31\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmb_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Loss & Backward\u001b[39;00m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/training/templates/problem_type/supervised_problem.py:49\u001b[0m, in \u001b[0;36mSupervisedProblem.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# use task-aware forward only for task-aware benchmarks\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mavalanche_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmb_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmb_task_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmb_x)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/models/utils.py:25\u001b[0m, in \u001b[0;36mavalanche_forward\u001b[0;34m(model, x, task_labels)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mavalanche_forward\u001b[39m(model, x, task_labels):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_multi_task_module(model):\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# no task labels\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model(x)\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/env_name/lib/python3.12/site-packages/avalanche/models/dynamic_modules.py:190\u001b[0m, in \u001b[0;36mMultiTaskModule.forward\u001b[0;34m(self, x, task_labels)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out_task\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, (\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti-head assumes mini-batches of 2 dimensions \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<batch, classes>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     n_labels_head \u001b[38;5;241m=\u001b[39m out_task\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 190\u001b[0m     out[task_mask, :n_labels_head] \u001b[38;5;241m=\u001b[39m out_task\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Starting experiment...\")\n",
    "results_dict = {}  # Use a dictionary instead of a list\n",
    "for index, experience in enumerate(benchmark.train_stream):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "    res = cl_strategy.train(experience)\n",
    "    print(\"Training completed\")\n",
    "    print(\"Computing accuracy on the whole test\")\n",
    "    results_dict[index] = cl_strategy.eval(benchmark.test_stream)  # Use the index as the key\n",
    "    if index == HPARAM[\"NUM_TASK\"]:\n",
    "        break\n",
    "\n",
    "print(\"Experiment completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "132d8917-ab73-4962-aa0a-7804be3ac2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    file_name = f\"{MODEL_NAME}_{DATASET_NAME}_{RUN}_results.txt\"\n",
    "    file_path = ROOT / DATASET_NAME / MODEL_NAME / RUN / file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "        file.write(f\"Dataset: {DATASET_NAME}\\n\")\n",
    "        file.write(f\"Run: {RUN}\\n\")\n",
    "        file.write(f\"Setting: {SETTING}\\n\")    \n",
    "        file.write(\"\\nResults Dictionary:\\n\")\n",
    "        file.write(\"--------------------------------------------------\\n\")\n",
    "        for key, value in results_dict.items():\n",
    "            file.write(f\"Experience {key}:\\n\")\n",
    "            for metric, metric_value in value.items():\n",
    "                # Convert tensors to lists for saving\n",
    "                if isinstance(metric_value, torch.Tensor):\n",
    "                    metric_value = metric_value.tolist()\n",
    "                file.write(f\"  {metric}: {metric_value}\\n\")\n",
    "            file.write(\"--------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_name)",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
